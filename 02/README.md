# 2. Nomad Simple Cluster

## Run Nomad Server and 2 Clients

Nomad 서버 및 클라이언트 2개를 실행한다. 

Vagrant로 구성했으며 서버 구성은 `cluster.yaml`에 작성하였듯이 Server 1대, Client 2대로 구성할 예정이다.

```
- name: nomad-server
  hostname: nomad-server
  box: bento/ubuntu-18.04
  ram: 1024
  ip: 172.17.8.101
- name: nomad-client01
  hostname: client-one
  box: bento/ubuntu-18.04
  ram: 1024
  ip: 172.17.8.102
- name: nomad-client02
  hostname: client-two
  box: bento/ubuntu-18.04
  ram: 1024
  ip: 172.17.8.103
```

Vagrant로 3개의 VM을 기동한다.  
```
vagrant up
```

server.hcl 구성 파일을 살펴보자. 에이전트를 `server`로 구성하고 클러스터에 하나의 서버를 구성함을 나타낸다. 실제 프로덕션 환경으로 가면 3-5개의 홀수개 클러스터로 구성을 권장한다. 그리고 API 및 서비스 간 통신을 위한 설정을 위해서 advertise 구문을 위에 설정한 IP로 작성하였다.  

```
# Increase log verbosity
log_level = "DEBUG"

# Setup data dir
data_dir = "/tmp/server1"

# Enable the server
server {
    enabled = true

    # Self-elect, should be 3 or 5 for production
    bootstrap_expect = 1
}

# Advertise an accessible IP address so the server is reachable by other servers
# and clients. The IPs can be materialized by Terraform or be replaced by an
# init script.
advertise {
    http = "172.17.8.101:4646"
    rpc = "172.17.8.101:4647"
    serf = "172.17.8.101:4648"
}
```

그런 다음 명령을 실행하여 백그라운드에서 Nomad 서버를 시작한다.  
```
nomad agent -config /vagrant/server.hcl > nomad.log 2>&1 &
[1] 64533
```

Nomad 서버의 PID를 확인할 수 있다. `cat nomad.log`를 통해 를 실행하여 로그를 확인할 수 있다. 여기에는 "Nomad agent started!"라는 메시지를 포함한 로그들을 확인할 수 있다.  

client1.hcl와 client2.hcl 구성 파일을 확인한다. 에이전트를 `client`로 구성하고 `servers = ["172.17.8.101:4647"]` 구성을 보면 클러스터에 바라보는 server의 통신은 RPC포트인 4647 구성함을 나타낸다. 그리고 server와 동일하게 API 및 서비스 간 통신을 위한 설정을 위해서 advertise 구문을 위에 설정한 각각의 IP로 작성하였다.  

```
# Increase log verbosity
log_level = "DEBUG"

# Setup data dir
data_dir = "/tmp/client"

# Enable the client
client {
    enabled = true

    # For demo assume we are talking to server1. For production,
    # this should be like "nomad.service.consul:4647" and a system
    # like Consul used for service discovery.
    servers = ["172.17.8.101:4647"]
}

# Advertise an accessible IP address so the server is reachable by other servers
# and clients. The IPs can be materialized by Terraform or be replaced by an
# init script.
advertise {
    http = "172.17.8.102:4646"
    rpc = "172.17.8.102:4647"
    serf = "172.17.8.102:4648"
}
```

client01에서 `client`를 실행한다.  
```
nomad agent -config /vagrant/client1.hcl > nomad.log 2>&1 &
[1] 21078
```

client02에서 `client`를 실행한다.  
```
nomad agent -config /vagrant/client2.hcl > nomad.log 2>&1 &
[1] 21111
```

두 `client` 모두 Nomad 서비스의 PID를 확인할 수 있다. `server`와 동일하게 `cat nomad.log`를 통해 를 실행하여 로그를 확인할 수 있다. 여기에는 "Nomad agent started!"라는 메시지를 포함한 로그들을 확인할 수 있다.  

## Run Nomad Server and 2 Clients

Create Your First Nomad Job
In this challenge, you will initialize a new Nomad job and then run it.

First, run the following command on the "Server" tab to generate the sample job specification file called example.nomad:

cd nomad
nomad job init -short
This will generate a minimal job specification file for our challenge. You may also run the this without the -short option to have a complete job specification that includes the comments for each section.

Click on the "Job Spec" tab and review the newly created example.nomad file.

The init command creates a sample job specification that deploys a Redis Docker image and demonstrates the use of task groups, tasks, Nomad's Docker driver, resource requirements, and the mapping of ports dynamically selected by Nomad to static ports used inside the redis Docker container.

The job specification is written in HCL, which aims to strike a balance between being human readable, editable, and machine-friendly.

We can see that the job is called "example", that it has a single task group called "cache", and that this task group has a single task called "redis" that uses the Docker driver to launch a standard redis image. We can also see that 500 MHz of CPU capacity and 256 MB of memory are required by the task and that a port called "db" dynamically generated by Nomad is mapped to port 6379 inside the Docker container.

If you look at the Jobs section of the Nomad UI, you will not yet see the new job. It will show up in the next challenge when you run it for the first time.


## Run and Monitor Your First Nomad Job
Run the Job
To get started, you will use the example.nomad job specification file that you created in the previous challenge. You can review it again on the "Job Spec" tab.

Recall that this job declares a single task, "redis" which uses Nomad's Docker driver to run the task.

Nomad jobs are planned with the nomad job plan command and run with the nomad job run command. The latter command is used both to register new jobs and to update existing jobs. In both cases, the job is sbumitted to a Nomad server which then schedules it on an available Nomad node.

It is common to skip planning a job the first time you run it since there is no risk of breaking what is not yet running.

Run the job with these commands on the "Server" tab:

cd nomad
nomad job run example.nomad
You will see a response from the Nomad server indicating that an evaluation has been created and that an allocation has been submitted to one of the client nodes. You will also see a message indicating that the evaluation has been completed.

Monitor the Job
A successful job submission is not an indication of a successfully-running job. A successful job submission means that the Nomad server was able to issue the proper scheduling commands; it does not indicate the job is actually running.

So, let's check the status of the job we just ran on the "Server" tab:

nomad job status example
This will show the ID of the job ("example"), the Status ("running"), the datacenters it is running in, and some other high-level information. It will then show a summary of the job's task groups, indicating how many are queued, started, running, failed, completed, and lost.

This is followed by information about the latest deployment of the job and by information about the job's allocations. You should see that one allocation is running and healthy.

Next, get more information about the evaluation by running:
nomad eval status <EvaluationID>
on the "Server" tab where <EvaluationID> is the ID of the evaluation displayed when you ran the job. (You can but don't need to wrap the evaluation ID in quotes.)

Now, get more information about the allocation by running:
nomad alloc status <AllocationID>
where <AllocationID> is the ID of the allocation returned when you ran the job. This will give you information about the "redis" task that was run including its IP and port and its recent events.

Finally, you should look at the logs for the task of an allocation. Do this for the "redis" task by running:
nomad alloc logs <AllocationID> redis
where <AllocationID> is the same allocation ID you used for the last command. You will see the complete logs from the Redis Docker container that was launched.

Note that while allocation logs can normally be viewed in the Nomad UI, this is blocked in the Instruqt environment.

We encourage you to also look at the status of the job and its allocation in the Nomad UI.

You can also look at deployment details on the "Deployments" tab, inspect allocations on the "Allocations" tab, and see evaluation details on the "Evaluations" tab.

In the next challenge, you will modify the example job specification to increase the task group count and run multiple instances of the Redis container.

## Modify a Job to Run More Instances
Since your Redis database has been getting a lot of traffic, you've decided to run more instances of it to handle the load.

Let's edit the example.nomad file on the "Job Spec" tab to set the count of the job's task group to 3. Do this immediately under the group stanza header and add a blank line after the new line so that you end up with:

group "cache" {
  count = 3
Once you have finished modifying the job specification, click the disk icon above the file to save it.

Change to the nomad directory on the "Server" tab before planning and running your modified job:

cd nomad
Next, use the nomad job plan command to invoke a dry-run of the Nomad scheduler to see what would happen if you ran the updated job:

nomad job plan example.nomad
You will see that the scheduler detected the change in count and informs us that it will cause 2 new allocations to be created. The in-place update that will occur pushes the update of the job specification to the existing allocation but will not cause any service interruption.

The dry-run indicates that there are enough resources for the modified job to be run by returning "All tasks successfully allocated."

Run the modified job with the following command:
nomad job run -check-index <job_modify_index> example.nomad
where <job_modify_index> is the job modify index returned by the nomad job plan command.

By using the nomad job run command with the check-index option and the job modify index from the previous plan output, we guarantee that running the job will do exactly what the plan showed.

After running the job, we can see that 2 additional allocations were created as desired.

Look at the job in the Nomad UI tab to verify that the additional allocations were deployed. You should see 3 healthy allocations. Also look at both clients in the Nomad UI and verify that one of them has 2 allocations and that the other has 1 allocation.

To stop the job and de-allocate all the redis docker containers, run the following command on the "Server" tab:

nomad job stop example
To verify that the job has completely stopped, run:

nomad status example

Within the output, confirm the following:
Check that the Status attribute of the job is marked as "dead (stopped)".
Under the Allocations section, verify that all three allocations have the "complete" status.

Congratulations on finishing the Nomad Simple Cluster track!